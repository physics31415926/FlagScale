name: Common All Tests

on:
  workflow_call:
    inputs:
      platform:
        required: true
        type: string
        description: Platform name (e.g., cuda, default)

jobs:
  checkout_and_config:
    defaults:
      run:
        shell: bash
    runs-on: ubuntu-latest
    outputs:
      ci_image: ${{ steps.config.outputs.ci_image }}
      runs_on: ${{ steps.config.outputs.runs_on }}
      container_volumes: ${{ steps.config.outputs.container_volumes }}
      container_options: ${{ steps.config.outputs.container_options }}
      device_types: ${{ steps.config.outputs.device_types }}
      train_test_matrix: ${{ steps.config.outputs.train_test_matrix }}
      hetero_train_test_matrix: ${{ steps.config.outputs.hetero_train_test_matrix }}
      inference_test_matrix: ${{ steps.config.outputs.inference_test_matrix }}
      rl_test_matrix: ${{ steps.config.outputs.rl_test_matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: false
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Cache source code
        id: cache-source
        uses: actions/cache@v4
        with:
          path: .
          key: source-code-${{ github.sha }}

      - name: Create source code tarball for distribution
        run: |
          tar -czf /tmp/flagscale-source.tar.gz \
            --exclude='.git' \
            --exclude='*.pyc' \
            --exclude='__pycache__' \
            .

      - name: Upload source code artifact
        uses: actions/upload-artifact@v4
        with:
          name: flagscale-source-${{ github.sha }}
          path: /tmp/flagscale-source.tar.gz
          retention-days: 1
          compression-level: 0

      - name: Load platform configuration
        id: config
        run: |
          set -euo pipefail

          PLATFORM="${{ inputs.platform }}"
          CONFIG_FILE=".github/configs/${PLATFORM}.yml"

          if [ ! -f "$CONFIG_FILE" ]; then
            echo "❌ Error: Platform configuration file not found: $CONFIG_FILE"
            echo "Available configs:"
            ls -la .github/configs/
            exit 1
          fi

          # Install mikefarah/yq (v4) for YAML parsing
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.45.1/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          /usr/local/bin/yq --version

          # Source the platform config loading script
          source ./tools/install/utils/load_platform_config.sh

          # Load configuration and group tests by task
          load_platform_config "$PLATFORM"

  unit_tests:
    needs: checkout_and_config
    strategy:
      fail-fast: false
      matrix:
        device: ${{ fromJson(needs.checkout_and_config.outputs.device_types) }}
    uses: ./.github/workflows/unit_tests_common.yml
    name: unit_tests
    with:
      platform: ${{ inputs.platform }}
      device: ${{ matrix.device }}
      image: ${{ needs.checkout_and_config.outputs.ci_image }}
      runs_on: ${{ needs.checkout_and_config.outputs.runs_on }}
      container_volumes: ${{ needs.checkout_and_config.outputs.container_volumes }}
      container_options: ${{ needs.checkout_and_config.outputs.container_options }}
      source_artifact: flagscale-source-${{ github.sha }}
      conda_env: flagscale-train  # Optional: can be empty for non-conda environments
      conda_path: "/root/miniconda3"  # Optional: specify custom conda path, empty for auto-detection

  functional_tests_train:
    needs:
      - checkout_and_config
      - unit_tests
    if: fromJson(needs.checkout_and_config.outputs.train_test_matrix)[0] != null
    uses: ./.github/workflows/functional_tests_train.yml
    with:
      platform: ${{ inputs.platform }}
      test_matrix: ${{ needs.checkout_and_config.outputs.train_test_matrix }}
      image: ${{ needs.checkout_and_config.outputs.ci_image }}
      runs_on: ${{ needs.checkout_and_config.outputs.runs_on }}
      container_volumes: ${{ needs.checkout_and_config.outputs.container_volumes }}
      container_options: ${{ needs.checkout_and_config.outputs.container_options }}
      source_artifact: flagscale-source-${{ github.sha }}
      conda_env: flagscale-train  # Optional: can be empty for non-conda environments
      conda_path: "/root/miniconda3"  # Optional: specify custom conda path, empty for auto-detection

  functional_tests_hetero_train:
    needs:
      - checkout_and_config
      - unit_tests
    if: fromJson(needs.checkout_and_config.outputs.hetero_train_test_matrix)[0] != null
    uses: ./.github/workflows/functional_tests_hetero_train.yml
    with:
      platform: ${{ inputs.platform }}
      test_matrix: ${{ needs.checkout_and_config.outputs.hetero_train_test_matrix }}
      image: ${{ needs.checkout_and_config.outputs.ci_image }}
      runs_on: ${{ needs.checkout_and_config.outputs.runs_on }}
      container_volumes: ${{ needs.checkout_and_config.outputs.container_volumes }}
      container_options: ${{ needs.checkout_and_config.outputs.container_options }}
      source_artifact: flagscale-source-${{ github.sha }}
      conda_env: flagscale-train  # Optional: can be empty for non-conda environments
      conda_path: "/root/miniconda3"  # Optional: specify custom conda path, empty for auto-detection

  functional_tests_inference:
    needs:
      - checkout_and_config
      - unit_tests
    if: fromJson(needs.checkout_and_config.outputs.inference_test_matrix)[0] != null
    uses: ./.github/workflows/functional_tests_inference.yml
    with:
      platform: ${{ inputs.platform }}
      test_matrix: ${{ needs.checkout_and_config.outputs.inference_test_matrix }}
      image: ${{ needs.checkout_and_config.outputs.ci_image }}
      runs_on: ${{ needs.checkout_and_config.outputs.runs_on }}
      container_volumes: ${{ needs.checkout_and_config.outputs.container_volumes }}
      container_options: ${{ needs.checkout_and_config.outputs.container_options }}
      source_artifact: flagscale-source-${{ github.sha }}
      conda_env: flagscale-inference  # Optional: can be empty for non-conda environments
      conda_path: "/root/miniconda3"  # Optional: specify custom conda path, empty for auto-detection

  functional_tests_rl:
    needs:
      - checkout_and_config
      - unit_tests
    if: fromJson(needs.checkout_and_config.outputs.rl_test_matrix)[0] != null
    uses: ./.github/workflows/functional_tests_rl.yml
    with:
      platform: ${{ inputs.platform }}
      test_matrix: ${{ needs.checkout_and_config.outputs.rl_test_matrix }}
      image: ${{ needs.checkout_and_config.outputs.ci_image }}
      runs_on: ${{ needs.checkout_and_config.outputs.runs_on }}
      container_volumes: ${{ needs.checkout_and_config.outputs.container_volumes }}
      container_options: ${{ needs.checkout_and_config.outputs.container_options }}
      source_artifact: flagscale-source-${{ github.sha }}
      conda_env: flagscale-rl  # Optional: can be empty for non-conda environments
      conda_path: "/root/miniconda3"  # Optional: specify custom conda path, empty for auto-detection

  all_tests_complete:
    defaults:
      run:
        shell: bash
    needs:
      - checkout_and_config
      - unit_tests
      - functional_tests_train
      - functional_tests_hetero_train
      - functional_tests_inference
      - functional_tests_rl
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Verify all tests passed
        run: |
          # Check all test jobs (skip if not run)
          failed=false

          if [ "${{ needs.unit_tests.result }}" != "success" ]; then
            echo "❌ Unit tests failed"
            failed=true
          fi

          # Only check functional tests if they ran
          if [ "${{ needs.functional_tests_train.result }}" != "success" ] && \
             [ "${{ needs.functional_tests_train.result }}" != "skipped" ]; then
            echo "❌ Training functional tests failed"
            failed=true
          fi

          if [ "${{ needs.functional_tests_hetero_train.result }}" != "success" ] && \
             [ "${{ needs.functional_tests_hetero_train.result }}" != "skipped" ]; then
            echo "❌ Heterogeneous training functional tests failed"
            failed=true
          fi

          if [ "${{ needs.functional_tests_inference.result }}" != "success" ] && \
             [ "${{ needs.functional_tests_inference.result }}" != "skipped" ]; then
            echo "❌ Inference functional tests failed"
            failed=true
          fi

          if [ "${{ needs.functional_tests_rl.result }}" != "success" ] && \
             [ "${{ needs.functional_tests_rl.result }}" != "skipped" ]; then
            echo "❌ RL functional tests failed"
            failed=true
          fi

          if [ "$failed" = "true" ]; then
            exit 1
          fi

          echo "✅ All tests completed successfully!"
